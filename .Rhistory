abline(h = 1, col = "black", lwd = 2)
legend("bottomright",legend = c("OLSCM", "HC0","HC1","HC2","HC3"),col = 1:5,pch = 16:20,
lty = 1,lwd = 2,box.lty = 1, box.lwd = 2)
par(mfrow = c(1, 1))
############################################### figure 2
## data 2
data_2 <- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, xd = xd, y = y1+tau(y1,exi,rs)*sqrt(x3+1.6)*exi)
pop_lm2 = lm( y1 ~ x1+x2+x3+x4, data = data_2)
beta_s = pop_lm2$coefficients[2:5]
figure_2 <- function(data, alpha) {
res = matrix(0, nrow = length(sample_size)*4, ncol = 5)
for (i in 1:length(sample_size)){
#critical value
cv <- c(qt(1 - alpha / 2, sample_size[i]-1),qnorm(0.975),qnorm(0.975),qnorm(0.975),qnorm(0.975))
for (j in 1:rep_n) {
#sample
sampled_data <- data[sample(1:nrow(data), sample_size[i], replace = FALSE), ]
ols.fit = lm( y ~ x1+x2+x3+x4, data = sampled_data )
ols.fit.hc0 = sqrt(diag( hccm ( ols.fit , type = "hc0" )))
ols.fit.hc1 = sqrt(diag( hccm ( ols.fit , type = "hc1" )))
ols.fit.hc2 = sqrt(diag( hccm ( ols.fit , type = "hc2" )))
ols.fit.hc3 = sqrt(diag( hccm ( ols.fit , type = "hc3" )))
ols.fit.coef = summary(ols.fit)$coef
for (b in 1:4){
tvalues = abs(ols.fit.coef[(b+1),1]-beta_s[b])/
cbind(ols.fit.coef[b+1,2],ols.fit.hc0[b+1],ols.fit.hc1[b+1],ols.fit.hc2[b+1],ols.fit.hc3[b+1])
res[i+length(sample_size)*(b-1),] <- res[i+length(sample_size)*(b-1),] + (tvalues>cv)
}
#tvalues_1 = abs(ols.fit.coef[2,1]-beta_s[1])/ cbind(ols.fit.coef[2,2],ols.fit.hc0[2],ols.fit.hc1[2],ols.fit.hc2[2],ols.fit.hc3[2])
#tvalues_2 = abs(ols.fit.coef[3,1]-beta_s[2])/ cbind(ols.fit.coef[3,2],ols.fit.hc0[3],ols.fit.hc1[3],ols.fit.hc2[3],ols.fit.hc3[3])
#tvalues_3 = abs(ols.fit.coef[4,1]-beta_s[3])/ cbind(ols.fit.coef[4,2],ols.fit.hc0[4],ols.fit.hc1[4],ols.fit.hc2[4],ols.fit.hc3[4])
#tvalues_4 = abs(ols.fit.coef[5,1]-beta_s[4])/ cbind(ols.fit.coef[5,2],ols.fit.hc0[5],ols.fit.hc1[5],ols.fit.hc2[5],ols.fit.hc3[5])
#
##size of test, beta = beta_s
#res[i,]     <- res[i,]    + (tvalues_1>cv)
#res[i+6,]   <- res[i+6,]  + (tvalues_2>cv)
#res[i+12,]  <- res[i+12,] + (tvalues_3>cv)
#res[i+18,]  <- res[i+18,] + (tvalues_4>cv)
}
}
return(res/rep_n)
}
res_2 <- figure_2(data_2, 0.05)
#plot
par(mfrow = c(2, 2))
for (b in 1:4){
x_axis <- 1:length(sample_size)
plot(x_axis, res_2[(length(sample_size)*(b-1)+1):(length(sample_size)*b),1],
type = "n", xaxt = "n", ylim = range(res_2[(length(sample_size)*(b-1)+1):(length(sample_size)*b),]),
xlab = "Sample Size", ylab = "Percent Rejected",main = paste('beta',b))
axis(1, at = x_axis, labels = sample_size)
for (i in 1:5) {
lines(x_axis, res_2[(length(sample_size)*(b-1)+1):(length(sample_size)*b),i], type = "o", col = i, pch = 15+i, lwd = 2)
}
abline(h = 0.05, col = "black", lwd = 2)
legend("topright",legend = c("OLSCM", "HC0","HC1","HC2","HC3"),col = 1:5,pch = 16:20,
lty = 0.5,lwd = 1,box.lty = 1, box.lwd = 2,cex = 0.5)
}
par(mfrow = c(1, 1))
############################################### figure 3
## data 3
data_3 <- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, xd = xd, y = y1+tau(y1,exi,rs)*sqrt(x3)*sqrt(x4+2.5)*exi)
figure_3 <- function(data, alpha) {
res = matrix(0, nrow = length(sample_size)*2, ncol = 5)
for (i in 1:length(sample_size)){
#critical value
cv <- c(qt(1 - alpha / 2, sample_size[i]-1),qnorm(0.975),qnorm(0.975),qnorm(0.975),qnorm(0.975))
for (j in 1:rep_n) {
#sample
sampled_data <- data[sample(1:nrow(data), sample_size[i], replace = FALSE), ]
ols.fit = lm( y ~ x1+x2+x3+x4, data = sampled_data )
ols.fit.hc0 = sqrt(diag( hccm ( ols.fit , type = "hc0" )))
ols.fit.hc1 = sqrt(diag( hccm ( ols.fit , type = "hc1" )))
ols.fit.hc2 = sqrt(diag( hccm ( ols.fit , type = "hc2" )))
ols.fit.hc3 = sqrt(diag( hccm ( ols.fit , type = "hc3" )))
ols.fit.coef = summary(ols.fit)$coef
for (b in c(1,3)){
tvalues = abs(ols.fit.coef[(b+1),1])/
cbind(ols.fit.coef[b+1,2],ols.fit.hc0[b+1],ols.fit.hc1[b+1],ols.fit.hc2[b+1],ols.fit.hc3[b+1])
res[i+length(sample_size)*(b-1)/2,] <- res[i+length(sample_size)*(b-1)/2,] + (tvalues>cv)
}
}
}
return(res/rep_n)
}
res_3 <- figure_3(data_3, 0.05)
#plot
par(mfrow = c(1, 2))
for (b in 1:2){
x_axis <- 1:length(sample_size)
plot(x_axis, res_3[(length(sample_size)*(b-1)+1):(length(sample_size)*b),1],
type = "n", xaxt = "n", ylim = range(0:1),
xlab = "Sample Size", ylab = "Percent Rejected",main = paste('Power for beta',b*2-1))
axis(1, at = x_axis, labels = sample_size)
for (i in 1:5) {
lines(x_axis, res_3[(length(sample_size)*(b-1)+1):(length(sample_size)*b),i], type = "o", col = i, pch = 15+i, lwd = 2)
}
abline(h = 0.05, col = "black", lwd = 2)
abline(h = 1, col = "black", lwd = 2)
legend("topleft",legend = c("OLSCM", "HC0","HC1","HC2","HC3"),col = 1:5,pch = 16:20,
lty = 0.5,lwd = 1,box.lty = 1, box.lwd = 2,cex = 0.5)
}
par(mfrow = c(1, 1))
############################################### figure 4
pop_lm4 = lm( y1 ~ x1+x2+x3+x4, data = data_3)
beta_3s = pop_lm4$coefficients[3]
figure_4 <- function(data, alpha) {
res = matrix(0, nrow = length(sample_size), ncol = 7) #white test siginificant or not; 5 test result
for (i in 1:length(sample_size)){
#critical value
cv <- c(qt(1 - alpha / 2, sample_size[i]-1),qnorm(0.975),qnorm(0.975),qnorm(0.975),qnorm(0.975))
for (j in 1:rep_n) {
#sample
sampled_data <- data[sample(1:nrow(data), sample_size[i], replace = FALSE), ]
ols.fit = lm( y ~ x1+x2+x3+x4, data = sampled_data )
ols.fit.hc0 = sqrt(diag( hccm ( ols.fit , type = "hc0" )))
ols.fit.hc1 = sqrt(diag( hccm ( ols.fit , type = "hc1" )))
ols.fit.hc2 = sqrt(diag( hccm ( ols.fit , type = "hc2" )))
ols.fit.hc3 = sqrt(diag( hccm ( ols.fit , type = "hc3" )))
ols.fit.coef = summary(ols.fit)$coef
tvalues = abs(ols.fit.coef[4,1]-beta_3s)/
cbind(ols.fit.coef[4,2],ols.fit.hc0[4],ols.fit.hc1[4],ols.fit.hc2[4],ols.fit.hc3[4])
#white test
white_test <- bptest(ols.fit, studentize = FALSE)
if (white_test$p.value < alpha){# significant, use HCCM-based test
res[i,1] <- res[i,1] + 1
res[i,3:6] <- res[i,3:6] + (tvalues>cv)[2:5]
}
#HC3
res[i,c(2,7)] <- res[i,c(2,7)] + (tvalues>cv)[c(1,5)]
}
}
res[,3:6] <- res[,3:6]/res[,1]
res[,c(1,2,7)] <- res[,c(1,2,7)]/rep_n
return(res)
}
res_4 <- figure_4(data_3, 0.05)
#plot
par(mfrow = c(1, 2))
#1
x_axis <- 1:length(sample_size)
plot(x_axis, res_4[,1], type = "n", xaxt = "n", ylim = range(c(0,1)),
xlab = "Sample Size", ylab = "Percent Rejected", main = 'White Test at .05 Level')
axis(1, at = x_axis, labels = sample_size)
lines(x_axis, res_4[,1], type = "o", col = 'black', pch = 15+i, lwd = 2)
abline(h = 0.05, col = "black", lwd = 2)
abline(h = 1, col = "black", lwd = 2)
#2
plot(x_axis, res_4[,6], type = "n", xaxt = "n", ylim = range(c(0,1)),
xlab = "Sample Size", ylab = "Power", main = 'Size of t-test after screening')
axis(1, at = x_axis, labels = sample_size)
for (i in 1:6) {
lines(x_axis, res_4[,i+1], type = "o", col = i, pch = 15+i, lwd = 2)
}
abline(h = 0.05, col = "black", lwd = 2)
abline(h = 1, col = "black", lwd = 2)
legend("bottomright",legend = c("OLSCM", "HC0","HC1","HC2","HC3(w/Screening)","HC3"),col = 1:6,pch = 16:21,
lty = 1,lwd = 2,box.lty = 1, box.lwd = 2,cex = 0.7)
par(mfrow = c(1, 1))
# Load necessary libraries
library(tensor)
library(mclust)
install.packages("tensor")
library(tensor)
library(mclust)
n <- 200
p <- c(20, 20)
u <- c(2, 2)
K <- 2
sigma <- 0.55
library(tensor)
library(mclust)
library(TensorClustering)
set.seed(123)
install.packages("pracma")    # For Frobenius norm computation
library(pracma)
set.seed(123)
# Function to generate tensor normal distributed data
generate_tensor_data <- function(n, K, p) {
# Initialize list to store tensors
tensors <- array(0, dim = c(n, p[1], p[2], p[3]))
cluster_labels <- sample(1:K, n, replace = TRUE)  # Assign clusters
# Generate cluster means and covariances
mu_list <- lapply(1:K, function(k) array(runif(prod(p), k, k+1), dim = p))
Sigma_list <- lapply(1:K, function(k) lapply(1:3, function(m) matrix(rnorm(p[m] * p[m], 0, 1), p[m], p[m])))
for (i in 1:n) {
k <- cluster_labels[i]
mu_k <- mu_list[[k]]
noise <- array(rnorm(prod(p), 0, 1), dim = p)  # Gaussian noise
tensors[i,,,] <- mu_k + noise
}
return(list(tensors = tensors, cluster_labels = cluster_labels, mu_list = mu_list))
}
# Frobenius norm function for error calculation
frobenius_norm <- function(A, B) {
return(sqrt(sum((A - B)^2)))
}
# Clustering using K-means (vectorized tensors)
kmeans_clustering <- function(tensors, K) {
n <- dim(tensors)[1]
p1 <- dim(tensors)[2]
p2 <- dim(tensors)[3]
p3 <- dim(tensors)[4]
# Vectorize tensors for k-means
tensor_vecs <- matrix(0, n, p1 * p2 * p3)
for (i in 1:n) {
tensor_vecs[i, ] <- as.vector(tensors[i,,,])
}
# Apply k-means clustering
kmeans_result <- kmeans(tensor_vecs, centers = K, nstart = 10)
return(kmeans_result$cluster)
}
# Non-parametric broadcasting function (e.g., apply nonlinear transformation)
broadcast_function <- function(tensor) {
return(sin(tensor) + cos(tensor))  # Example transformation
}
# Main simulation function
run_simulation <- function(n, K, p) {
# Generate data
data <- generate_tensor_data(n, K, p)
tensors <- data$tensors
true_labels <- data$cluster_labels
true_means <- data$mu_list
# Apply broadcasting transformation
broadcasted_tensors <- array(apply(tensors, c(2,3,4), broadcast_function), dim = c(n, p[1], p[2], p[3]))
# Perform clustering
kmeans_labels <- kmeans_clustering(tensors, K)
# Compute Frobenius norm error for mean estimation
estimated_means <- lapply(1:K, function(k) {
mean_tensor <- apply(tensors[kmeans_labels == k,,,], c(2,3,4), mean)
return(mean_tensor)
})
frobenius_errors <- sapply(1:K, function(k) frobenius_norm(estimated_means[[k]], true_means[[k]]))
# Results
return(list(
kmeans_labels = kmeans_labels,
true_labels = true_labels,
frobenius_errors = frobenius_errors
))
}
# Run simulation for n = 100
set.seed(42)
result <- run_simulation(n = 100, K = 2, p = c(10,10,10))
# Print Frobenius norm errors
print(result$frobenius_errors)
# Evaluate clustering accuracy
accuracy <- sum(result$kmeans_labels == result$true_labels) / length(result$true_labels)
print(paste("Clustering Accuracy:", round(accuracy * 100, 2), "%"))
library(doParallel)
# Detect available cores
num_cores <- detectCores() - 1
# Create a cluster and register it
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Parallel computation with a fixed seed per iteration
result <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
set.seed(123 + i)  # Ensure different seed per iteration
i^2
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
# Detect available cores
num_cores <- detectCores() - 1
# Create a cluster and register it
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Parallel computation with a fixed seed per iteration
result <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
set.seed(123 + i)  # Ensure different seed per iteration
i^2
}
# Stop the cluster
stopCluster(cl)
print(result)
detectCores() - 1
library(doParallel)
# Function that generates a random number (with fixed seed)
random_function <- function(i) {
set.seed(123 + i)  # Unique seed per iteration
runif(1)  # Generate a random number
}
# Detect available cores
num_cores <- detectCores()
# Create a cluster and register it
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Run parallel computation with a fixed seed per iteration
result <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
random_function(i)
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
# Function that generates a random number (with fixed seed)
random_function <- function(i) {
set.seed(123 + i)  # Unique seed per iteration
runif(1)  # Generate a random number
}
# Detect available cores
num_cores <- detectCores()
# Create a cluster and register it
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Run parallel computation with a fixed seed per iteration
result <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
random_function(i)
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
# Function that generates a random number (with fixed seed)
random_function <- function(i) {
set.seed(123 + i)  # Unique seed per iteration
runif(1)  # Generate a random number
}
# Detect available cores
num_cores <- detectCores()
# Create a cluster and register it
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Run parallel computation with a fixed seed per iteration
result <- foreach(i = 1:3, .combine = c) %dopar% {
random_function(i)
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
random_function <- function(i) {
set.seed(123 + i)
runif(1)
}
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Run parallel computation with a fixed seed per iteration
result <- foreach(i = 1:3) %dopar% {
random_function(i)
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
random_function <- function(i) {
set.seed(123 + i)
runif(1)
}
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Run parallel computation with a fixed seed per iteration
result <- foreach(i = 1:3) %dopar% {
random_function(i)
}
# Stop the cluster
stopCluster(cl)
print(result)
library(doParallel)
random_function <- function(i) {
set.seed(123 + i)
runif(1)
}
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
result <- foreach(i = 1:3, .combine = c) %dopar% {
random_function(i)
}
stopCluster(cl)
print(result)
library(doParallel)
# Define a computationally expensive function
expensive_function <- function(i) {
set.seed(123 + i)  # Fix seed for reproducibility
sum(runif(10^6))   # Sum of 1 million random numbers
}
# -----------------------
# 1. Run Without Parallelization
# -----------------------
start_time_seq <- Sys.time()
result_seq <- sapply(1:10, expensive_function)
end_time_seq <- Sys.time()
time_seq <- end_time_seq - start_time_seq
print(paste("Sequential Execution Time:", time_seq))
# -----------------------
# 2. Run With Parallelization
# -----------------------
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores)
registerDoParallel(cl)
start_time_par <- Sys.time()
result_par <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
expensive_function(i)
}
end_time_par <- Sys.time()
stopCluster(cl)
time_par <- end_time_par - start_time_par
print(paste("Parallel Execution Time:", time_par))
library(doParallel)
expensive_function <- function(i) {
set.seed(123 + i)  # Fix seed
sum(runif(10^6))
}
# -----------------------
# 1. Run Without Parallelization
# -----------------------
start_time_seq <- Sys.time()
result_seq <- sapply(1:10, expensive_function)
end_time_seq <- Sys.time()
time_seq <- end_time_seq - start_time_seq
print(paste("Sequential Execution Time:", time_seq))
# -----------------------
# 2. Run With Parallelization
# -----------------------
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
start_time_par <- Sys.time()
result_par <- foreach(i = 1:10, .combine = c, .packages = "base") %dopar% {
expensive_function(i)
}
end_time_par <- Sys.time()
stopCluster(cl)
time_par <- end_time_par - start_time_par
print(paste("Parallel Execution Time:", time_par))
library(rTensor)
library(doParallel)
library(foreach)
# Define CP decomposition function with a fixed seed
cp_decomp <- function() {
set.seed(123)  # Fix seed for reproducibility
tensor_data <- array(runif(4 * 5 * 6), dim = c(4, 5, 6))  # Create a tensor
tensor <- as.tensor(tensor_data)
cp(tensor, num_components = 3)  # Perform CP decomposition
}
# ----------------------
# 1. Sequential Execution
# ----------------------
start_time_seq <- Sys.time()
result_seq <- cp_decomp()  # Run sequentially
end_time_seq <- Sys.time()
time_seq <- end_time_seq - start_time_seq
print(paste("Sequential Execution Time:", time_seq))
# ----------------------
# 2. Parallel Execution
# ----------------------
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores)
registerDoParallel(cl)
start_time_par <- Sys.time()
# Perform CP decomposition in parallel
result_par <- foreach(i = 1:10, .packages = "rTensor") %dopar% {
cp_decomp()
}
end_time_par <- Sys.time()
stopCluster(cl)
time_par <- end_time_par - start_time_par
print(paste("Parallel Execution Time:", time_par))
# ----------------------
# 3. Verify Results are Identical
# ----------------------
identical_result <- identical(result_seq, result_par[[1]])  # Compare first parallel result with sequential
print(paste("Are results identical?", identical_result))
# Define CP decomposition function with a fixed seed
# Define CP decomposition function with a fixed seed
cp_decomp <- function(i) {
set.seed(123 + i)  # Fix seed per iteration
tensor_data <- array(runif(4 * 5 * 6), dim = c(4, 5, 6))  # Create a tensor
tensor <- as.tensor(tensor_data)
cp(tensor, num_components = 3)  # Perform CP decomposition
}
# Number of times to run CP decomposition
num_runs <- 10
# ----------------------
# 1. Sequential Execution (Fair Comparison)
# ----------------------
start_time_seq <- Sys.time()
result_seq <- lapply(1:num_runs, cp_decomp)  # Run the same number of times as parallel
end_time_seq <- Sys.time()
time_seq <- end_time_seq - start_time_seq
print(paste("Sequential Execution Time:", time_seq))
# ----------------------
# 2. Parallel Execution
# ----------------------
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores)
registerDoParallel(cl)
start_time_par <- Sys.time()
# Perform CP decomposition in parallel
result_par <- foreach(i = 1:num_runs, .packages = "rTensor") %dopar% {
cp_decomp(i)
}
end_time_par <- Sys.time()
stopCluster(cl)
time_par <- end_time_par - start_time_par
print(paste("Parallel Execution Time:", time_par))
# ----------------------
# 3. Verify Results are Identical
# ----------------------
identical_results <- all(mapply(identical, result_seq, result_par))  # Compare all results
print(paste("identical", identical_results))
update
setwd("C:/Users/yh95l/Desktop/CMTE")
unlink(list.files("Data", full.names = TRUE), recursive = TRUE, force = TRUE)
file.remove(list.files("results", full.names = TRUE))
source("./parameters.R")
source("./DataGen.R")
unlink(list.files("Data", full.names = TRUE), recursive = TRUE, force = TRUE)
